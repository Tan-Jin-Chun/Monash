{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a039ea3c",
   "metadata": {
    "id": "a039ea3c"
   },
   "source": [
    "$\\color{green}{\\text{This notebook is best viewed in jupyter lab/notebook. You may also choose to use Google Colab but some parts of the images/colouring will not be rendered properly.}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64d2560",
   "metadata": {
    "id": "d64d2560"
   },
   "source": [
    "# Lab 3 (Weeks 8,9): k-Means Clustering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a5d556f6",
   "metadata": {
    "id": "a5d556f6"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "<b>Enter you credentials below</b>\n",
    "\n",
    "- <b>Student Name:</b> Jin Chun Tan\n",
    "- <b>Student ID:</b> 32194471\n",
    "- <b>Date:</b> March 21, 2023\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99add4fa",
   "metadata": {
    "id": "99add4fa"
   },
   "source": [
    "![unsup2.jpg](img/ML.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9636a525",
   "metadata": {
    "id": "9636a525"
   },
   "source": [
    "## Unsupervised Learning\n",
    "\n",
    "As the name suggests, unsupervised learning is a type of machine learning in which the training of a model is not supervised using labels of the training dataset. Instead, models themselves attempt to find hidden patterns and insights from the given data. Unsupervised learning cannot be directly applied to a regression or classification problem because unlike supervised learning, we have the input data but no corresponding output data (no labels). The goal of unsupervised learning is to find the underlying structure of the dataset, group that data according to similarities, and represent the data in a compressed format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e68d739",
   "metadata": {
    "id": "2e68d739"
   },
   "source": [
    "## Clustering\n",
    "\n",
    "Clustering is a type of unsupervised learning which involves grouping of data points. Given a set of data points, we can use a clustering algorithm to classify each data point into a specific group. In theory, data points that are in the same group should have similar properties and/or features, while data points in different groups should have highly dissimilar properties and/or features. Some examples of popular clustering algorithms are:\n",
    "- __K-Means Clustering__\n",
    "- __Mean-Shift Clustering__\n",
    "- Spectral Clustering\n",
    "- DBSCAN\n",
    "- Gaussian Mixture Models (GMM)\n",
    "\n",
    "In this lab, we will be implementing k-means clustering and mean-shift clustering on the same dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5706c7f",
   "metadata": {
    "id": "a5706c7f"
   },
   "source": [
    "## K-Means Clustering\n",
    "\n",
    "K-Means Clustering is one of the simplest unsupervised machine learning algorithms. You will first decide a number *k*, which corresponds to the number of clusters that you desire to have in your dataset. This number will also correspond to the number of centroids. A centroid is the location representing the center of a cluster. Each data point in your dataset is allocated to a specific cluster following a set of rules in an iterative manner. Also, the locations of the centroids are updated subsequently by averaging the data points assigned to the respective cluster. The **‘means’** in the k-Means Clustering refers exactly to this method of updating each cluster centroid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49ed2a9",
   "metadata": {
    "id": "c49ed2a9"
   },
   "source": [
    "![kmeans_anim.gif](img/kmeans_anim.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948cb0aa",
   "metadata": {
    "id": "948cb0aa"
   },
   "source": [
    "## What you should do in this lab exercise!\n",
    "\n",
    "In this laboratory exercise, you will create a program that clusters and re-colours each pixel in a provided colour image to **k** mean colours using the k-means clustering algorithm. In all of the tasks below, you may not use any pre-written libraries for k-Means Clustering (e.g. **no** scikit-learn), instead you should use your knowledge of python and numpy to build your own code to perform k-Means Clustering. Your end result of the clustered image should look something like the one shown below.\n",
    "\n",
    "*Note*: If you cannot see the displayed image '*objective_sharon.jpg*', have a look into the provided '*img*' folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd699d9a",
   "metadata": {
    "id": "dd699d9a"
   },
   "source": [
    "![objective_sharon.jpg](img/objective_sharon.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e5729d",
   "metadata": {
    "id": "32e5729d"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "You are going to work on the following six tasks throughout this lab. <span style=\"color:red\"> </span>\n",
    "\n",
    "- **Task 1** : Distance Function\n",
    "- **Task 2** : k-Means Clustering and visualisation\n",
    "- **Task 3** : k-Means++ Initialization\n",
    "- **Task 4** : GMM: Analysing Data Distributions.\n",
    "- **Task 5** : Mean Shift - Data Modelling via its distribution.\n",
    "- **Task 6** : Comparison between k-Means, k-Means++ Initialization and Mean-Shift.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5889c40e",
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1649292640794,
     "user": {
      "displayName": "Markus Hiller",
      "userId": "16727612601438706693"
     },
     "user_tz": -600
    },
    "id": "5889c40e"
   },
   "outputs": [],
   "source": [
    "# As always, we first import several libraries that will be helpful to solve the tasks\n",
    "# Important: You are only allowed to use cv2 to import images, but you may NOT use the contained k-means functionality\n",
    "# For the GMM, you will run through the steps of understanding the model, then use a library to apply it to the same image from the k-means task\n",
    "\n",
    "import matplotlib.pyplot as plt  \n",
    "from mpl_toolkits import mplot3d\n",
    "import numpy as np     \n",
    "import cv2\n",
    "import time\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from matplotlib.colors import ListedColormap\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bece237",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "## Task 1: Distance function\n",
    "In this task, you wil build a helper function to compute the squared distance from a set of data points to a set of mean values (*aka* centroids). You are given a small dataset of five data points and two centroids for testing. Using your 'dist2c' function, you will compute distances from each centroid to every data point. Finally, you will visualize your results and check whether your distances are correct by assigning the data points to the closest centroid and displaying the results.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c627b64",
   "metadata": {
    "id": "5c627b64"
   },
   "source": [
    "Now, write your own **dist2c** function, which takes a set of data points ('*data*') and a set of means ('*centroids*') and computes the squared distance from each mean to every data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85831bc1",
   "metadata": {
    "id": "85831bc1"
   },
   "outputs": [],
   "source": [
    "def dist2c(data, centroids):\n",
    "    # The inputs and the outputs of your function should be as follows:\n",
    "    # Inputs - data      : numpy array of size N x d\n",
    "    #          centroids : numpy array of size c x d\n",
    "    # Output - dist      : numpy array of size c x N\n",
    "    # N = the number of data points, c = the number of centroids, d = dimension of data\n",
    "\n",
    "    ### Insert your solution here ###\n",
    "    # data[:, np.newaxis, :] command willl select all rows of the 'data' array (:) and creates a new axis at position 1 using np.newaxis function\n",
    "    # which transforms the shapre of 'data' from '(n_samples, n_features)' to '(n_samples,1,n_features)'\n",
    "\n",
    "    # We will need to subtract it with centroids with the data array \n",
    "    dist = np.sum((data[:, np.newaxis, :] - centroids) ** 2, axis=2)\n",
    "    return dist.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a255856b",
   "metadata": {
    "id": "a255856b"
   },
   "source": [
    "Now, let's check the function you wrote on the following provided data points.\n",
    "- X contains five 3-dimensional data points\n",
    "\n",
    "\\begin{equation}\n",
    "X = \n",
    "\\begin{bmatrix}\n",
    "0.67187976 & 0.44254368 & 0.17900127\\\\\n",
    "0.55085456 & 0.65891464 & 0.18370379\\\\\n",
    "0.79861987 & 0.3439561  & 0.68334744\\\\\n",
    "0.36695437 & 0.15391793 & 0.81100023\\\\\n",
    "0.22898267 & 0.58062367 & 0.5637733 \n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "- M contains two centroids\n",
    "\\begin{equation}\n",
    "M = \n",
    "\\begin{bmatrix}\n",
    "0.66441854 & 0.08332493 & 0.54049661\\\\\n",
    "0.05491067 & 0.94606233 & 0.29515262\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Use your **dist2c** function to compute the distances from each centroid to every data point. Print your results. If you wrote the function correctly, your answer should be close to:\n",
    "\n",
    "\\begin{bmatrix}\n",
    "0.25977266 & 0.47150141 & 0.10634496 & 0.16664051 & 0.43745224\\\\\n",
    "0.64767303 & 0.34083498 & 1.0663305 & 0.99096278 & 0.23600355\n",
    "\\end{bmatrix}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5205b29",
   "metadata": {
    "id": "b5205b29",
    "outputId": "1f65ac95-0ec3-4731-ef1a-9f600315ae49"
   },
   "outputs": [],
   "source": [
    "### Insert your solution here ###\n",
    "# Initialising the variable\n",
    "X = np.array([\n",
    "    [0.67187976, 0.44254368, 0.17900127],\n",
    "    [0.55085456, 0.65891464, 0.18370379],\n",
    "    [0.79861987, 0.3439561, 0.68334744],\n",
    "    [0.36695437, 0.15391793, 0.81100023],\n",
    "    [0.22898267, 0.58062367, 0.5637733]\n",
    "])\n",
    "\n",
    "M = np.array([\n",
    "    [0.66441854, 0.08332493, 0.54049661],\n",
    "    [0.05491067, 0.94606233, 0.29515262]\n",
    "])\n",
    "\n",
    "# Calling the function dist2c that we have created from above\n",
    "dist = dist2c(X,M)\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34060b45",
   "metadata": {
    "id": "34060b45"
   },
   "source": [
    "Write a code to draw a 3D scatter plot assigning each datapoint to its closest centroid using two colors.\n",
    "\n",
    "*Hint*: Use the '*scatter3D*' function from matplotlib, and choose a different marker (or different marker size) to indicate the cluster centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a1bf5f",
   "metadata": {
    "id": "53a1bf5f",
    "outputId": "03ab9be9-5aca-436b-9e84-3923f47a5d92"
   },
   "outputs": [],
   "source": [
    "### Insert your solution here ###\n",
    "# Assign each point to closest centroid\n",
    "labels = np.argmin(dist.T, axis=1)\n",
    "\n",
    "# Create 3D scatter plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot data points with different colors based on their closest centroid\n",
    "colors = ['red', 'blue']\n",
    "for i in range(len(X)):\n",
    "    ax.scatter3D(X[i, 0], X[i, 1], X[i, 2], c = colors[labels[i]])\n",
    "\n",
    "# # Plot centroids with black 'x' markers\n",
    "ax.scatter3D(M[:, 0], M[:, 1], M[:, 2], c = 'black', marker = 'x')\n",
    "\n",
    "# Label x, y, and z axes\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "ax.set_title(\"3D Scatter Plot\")\n",
    "\n",
    "# Show the new figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03d2c9e",
   "metadata": {
    "id": "f03d2c9e"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "#### What do you observe here? Are the data points assigned to their closest centroid?\n",
    "\n",
    "Write your answer here\n",
    "There are two clusters of points, each centered around a black 'x' marker (which represents the centroid). Each cluster corresponds to the points assigned to one of the centroids.\n",
    "\n",
    "The points in each cluster should be closer to their respective centroid than to the other centroid. The data points that are closer to a \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538a0eb0",
   "metadata": {
    "id": "538a0eb0"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "## Task 2: K-means Clustering\n",
    "In this task, you will implement k-Means Clustering. First, you will write a function to generate random centroids for initialization. Then you will write a function to perform k-Means Clustering (You may want to follow the steps on the lab instructions sheet). Afterwards, you will use these functions to cluster the pixels of the provided *sharon.jpg* image. Finally, you will display the results.    \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9ac451",
   "metadata": {
    "id": "7c9ac451"
   },
   "source": [
    "Write a function **random_centroids**, which takes a dataset and an integer value k (= number of centroids) as inputs, and generates k number of _centroids_ which are randomly sampled from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecea901",
   "metadata": {
    "id": "0ecea901"
   },
   "outputs": [],
   "source": [
    "def random_centroids(data, k):\n",
    "    # The inputs and the outputs of your function should be as follows:\n",
    "    # Inputs - data     : numpy array (N x d)\n",
    "    #        - k        : an integer value\n",
    "    # Output - centroids: numpy array (k x d)\n",
    "    # N = number of data points, d = dimension of data, k = number of centroids\n",
    "    \n",
    "    ### Insert your solution here ###\n",
    "    # This function will take a 2D numpy array 'data' of shape (n,d) where n is the number of data points and 'd' is the number of features\n",
    "    # and an integer 'k' which is the number of centroids to generate.\n",
    "    #\n",
    "    # It returns a 2D numpy array of shape (k,d) where each row is a centroid\n",
    "    \n",
    "    # Testing Code\n",
    "    # indices = np.random.choice(data.shape[0], size=k, replace=False)\n",
    "    # centroids = data[indices, :]\n",
    "    \n",
    "    # Finding the dataset \n",
    "    features = data.shape[1]\n",
    "\n",
    "    # Initializing an array of empty centroids for efficiency allocation purposes\n",
    "    centroids = np.zeros((k, features))\n",
    "\n",
    "    # Randomization of k points from the dataset\n",
    "    indices = np.random.choice(data.shape[0], size = k, replace = False)\n",
    "    for i, index in enumerate(indices):\n",
    "        centroids[i] = data[index]\n",
    "\n",
    "    # Returning a 2D numpy array of centroids\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7e331d",
   "metadata": {
    "id": "be7e331d"
   },
   "source": [
    "Write a function **mykMeans**, which takes a dataset, an set of k centroids and an integer value T (= number of iterations) as inputs. The function should perform k-Means Clustering on the input dataset by using the input centroids as intilization. After running for T iterations, the function should finally output a cluster index for each data point, the final set of k centroids, and the loss vector containing the k-Means loss at each iteration. (You may want to use your function **dist2c** to compute distances between centroids and data points at each iteration.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d9eae4",
   "metadata": {
    "id": "d5d9eae4"
   },
   "outputs": [],
   "source": [
    "def mykMeans(data, centroids, T):\n",
    "    # The inputs and the outputs of your function should be as follows:\n",
    "    # Inputs  - data        : numpy array (N x d)\n",
    "    #         - centroids   : numpy array (k x d)\n",
    "    #         - T           : integer (the number of iterations)\n",
    "    # Outputs - cluster_idx : numpy array (N,)\n",
    "    #         - centroids   : numpy array (k x d)\n",
    "    #         - losses      : list (T)\n",
    "    # N = number of data points, d = dimension of data, k = number of centroids, T = number of iterations\n",
    "\n",
    "    # Initialise the arrays to store the values for the centroids\n",
    "    sum_loss = np.zeros((data.shape[0], 1))\n",
    "    sum_dimloss = np.zeros((1, data.shape[1]))\n",
    "\n",
    "    # Initialize the list to store the k-means loss at each iteration\n",
    "    kMeans_loss = np.zeros((1,T))\n",
    "\n",
    "    for t in range(T):\n",
    "\n",
    "        # Reinitialising all of the required values\n",
    "        distances = dist2c(data, centroids)\n",
    "        cluster_idx = np.argmin(distances.T,axis = 1)\n",
    "\n",
    "        # Step 1: Go through all the data points and calculate which of the k centroids it is closest to.\n",
    "        for i in range(centroids.shape[0]):\n",
    "            indexes = np.where(cluster_idx == i)\n",
    "            centroids[i] = np.mean(data[indexes[0]],axis = 0)\n",
    "\n",
    "        # Step 2: For each data point, store the index of the nearest centroid at the same location in an index image.\n",
    "        # (This step is already completed in the loop above)\n",
    "\n",
    "        # Step 3: Re-compute each centroid by going through all the data points that were assigned to that centroid and taking the mean.\n",
    "        for j in range(data.shape[0]):\n",
    "\n",
    "            # Calculate the k-means loss at this iteration\n",
    "            for k in range(centroids.shape[0]):\n",
    "\n",
    "                # Checking for the cluster_idx\n",
    "                if cluster_idx[j] == k:\n",
    "\n",
    "                    for index in range(data.shape[1]):\n",
    "                        sum_dimloss[0][index] = (data[j][index] - centroids[k][index]) ** 2\n",
    "                    sum_loss[j][0] = np.sum(sum_dimloss)\n",
    "\n",
    "        kMeans_loss[0][t] = np.sum(sum_loss)\n",
    "\n",
    "    # Return back the three values\n",
    "    return cluster_idx, centroids, kMeans_loss\n",
    "\n",
    "    ## Alternative Way of doing it\n",
    "    # # Initialize the array to store the index of the nearest centroid for each data point\n",
    "    # cluster_idx = np.zeros(N, dtype=int)\n",
    "\n",
    "    # N = data.shape[0]\n",
    "    # kMeans_loss = np.zeros(T)\n",
    "\n",
    "    # # Loop through the specified number of iterations\n",
    "    # for t in range(T):\n",
    "    #     # Calculate the distances between data points and centroids\n",
    "    #     distances = dist2c(data, centroids)\n",
    "        \n",
    "    #     # Assign each data point to the closest centroid\n",
    "    #     cluster_idx = np.argmin(distances, axis=1)\n",
    "        \n",
    "    #     # Initialize the loss for the current iteration\n",
    "    #     loss = 0\n",
    "        \n",
    "    #     # Update the centroids and calculate the loss\n",
    "    #     for i in range(centroids.shape[0]):\n",
    "    #         # Find the indices of data points assigned to the current centroid\n",
    "    #         indexes = np.where(cluster_idx == i)\n",
    "            \n",
    "    #         # Update the centroid only if there are data points assigned to it\n",
    "    #         if len(indexes[0]) > 0:\n",
    "    #             # Calculate the new centroid as the mean of the assigned data points\n",
    "    #             centroids[i] = np.mean(data[indexes[0]], axis=0)\n",
    "                \n",
    "    #             # Add the squared distance between the assigned data points and the centroid to the loss\n",
    "    #             loss += np.sum((data[indexes[0]] - centroids[i])**2)\n",
    "        \n",
    "    #     # Store the loss for the current iteration\n",
    "    #     kMeans_loss[t] = loss\n",
    "\n",
    "    # # Return the final cluster indices, updated centroids, and k-means loss at each iteration\n",
    "    # return cluster_idx, centroids, kMeans_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1224e1",
   "metadata": {
    "id": "1b1224e1"
   },
   "source": [
    "Load the *sharon.jpg* image and display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774ba696",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Insert your solution here ###\n",
    "\n",
    "# Load and display the colour (!) image\n",
    "# Loading the image\n",
    "img = plt.imread('sharon.jpg')\n",
    "\n",
    "# Displaying the image\n",
    "plt.imshow(img)\n",
    "plt.title('Original Image')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfb5054",
   "metadata": {
    "id": "5cfb5054"
   },
   "source": [
    "Now, use your **random_centroids** function to generate four random centroids. Then, use your **mykMeans** function to cluster the pixels of the *sharon.jpg* image by running it for 20 iterations (T=20). (*Hint*: You may want to reshape your image before applying the **mykMeans** function.) </br> Display the final clustered image. Each pixel should be displayed by the colour of the assigned centroid (i.e. you should only see a total of *k* colours in your clustered image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340113cb",
   "metadata": {
    "id": "340113cb",
    "outputId": "2a122cf2-d300-4b22-91ca-7fe7d7c122b4"
   },
   "outputs": [],
   "source": [
    "# Define a fixed random seed for repeatability\n",
    "np.random.seed(17)\n",
    "\n",
    "### Insert your solution here ###\n",
    "# Initialising the variables\n",
    "data = img[:][1]\n",
    "\n",
    "# Looping through the image to fill in our data variable\n",
    "for i in range(img.shape[1]-1):\n",
    "    data = np.append(data,img[:][i+1], axis = 0)\n",
    "\n",
    "# Calling our random centroids function to generate 4 random centroids\n",
    "random_centroides = random_centroids(data,4)\n",
    "\n",
    "# Calling the mykMeans function\n",
    "cluster_idx, centroids, kMeans_loss = mykMeans(data, random_centroides, T = 20)\n",
    "\n",
    "# Displaying the image\n",
    "new_matrix = np.zeros((65536,3))\n",
    "\n",
    "# Using a for loop to fill up our clustered_img with the appropriate vales\n",
    "for i in range(centroids.shape[0]):\n",
    "    indexes = np.where(cluster_idx == i)\n",
    "    new_matrix[indexes[0]] = centroids[i]\n",
    "\n",
    "# Reshaping the image\n",
    "clustered_img = new_matrix.reshape(img.shape)\n",
    "clustered_img = np.around(clustered_img).astype(int)\n",
    "\n",
    "# Displaying the image\n",
    "plt.imshow(clustered_img)\n",
    "plt.title('Clustered Image')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3571c5a",
   "metadata": {
    "id": "c3571c5a"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "#### Have some fun with your k-Means Clustering impementation! Run the program several times with different random seeds to see if you always converge to the same solution. Try changing k from 4 to other numbers (from 2 to 10) and see how this affects the output and the repeatability of the program. Report and discuss your observations.\n",
    "\n",
    "\n",
    "Write your answer here\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446a3f06",
   "metadata": {},
   "source": [
    "Now, visualize how the centroids are updated during k-Means Clustering. For that, you should write a code to plot the data points along with the centroids and display both the plot and the clustered image side-by-side. Then, use that function inside your k-Means Clustering implementation to observe how the centroids move with each iteration. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82815d38",
   "metadata": {
    "id": "82815d38"
   },
   "source": [
    "Write your own function **visualize** which takes the dataset, cluster indices, centroids and sample indices, and generates a figure of two subplots:\n",
    "1. A 3D scatter plot of the datapoints and centroids.\n",
    "2. A display of the clustered image\n",
    "\n",
    "For the 3D scatter plot, the colour of each data point should reflect the colour of the closest centroid (i.e. the centroid which it is assigned to). As it is computationally expensive to plot all the data points at each iteration, you may only draw 250 randomly selected data points (to speed things up)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07184424",
   "metadata": {
    "id": "07184424"
   },
   "outputs": [],
   "source": [
    "def visualize(data, cluster_idx, centroids, sample_idx):\n",
    "    # Inputs  - data        : numpy array (N x d)\n",
    "    #         - cluster_idx : numpy array (N,)\n",
    "    #         - centroids   : numpy array (k x d)\n",
    "    #         - sample_idx  : numpy array (250,)\n",
    "    # Outputs - figure      : subplot (1, 2)\n",
    "    # N = number of data points, d = dimension of data, k = number of centroids\n",
    "    \n",
    "    # Initialising the variables\n",
    "    split_data = np.split(data,256)\n",
    "    data_reframed = np.stack(split_data, axis = 1)\n",
    "\n",
    "    split_cluster_idx = np.split(cluster_idx, 256)\n",
    "    cluster_idx = np.stack(split_cluster_idx, axis = 1)\n",
    "\n",
    "    # Create 3D scatter plot with corresponding colors as first sub-plot\n",
    "    plt.figure()\n",
    "    plt.rcParams['figure.figsize'] = [30,30]\n",
    "    plt.subplot(1,2,1, projection='3d')\n",
    "\n",
    "    # Looping through the sample_idx variable\n",
    "    for i in range(sample_idx.shape[0]):\n",
    "        colour = centroids[cluster_idx[sample_idx[i][0]][sample_idx[i][1]]]\n",
    "        x_points = sample_idx[i][0]\n",
    "        y_points = sample_idx[i][1]\n",
    "        new_set = data_reframed[x_points][y_points]\n",
    "        plt.scatter(new_set[2],new_set[1],new_set[0], color = [colour[2]/255.0 , colour[1]/255.0, colour[0]/255.0])\n",
    "\n",
    "    # Looping through the centroids \n",
    "    for j in range(centroids.shape[0]):\n",
    "        plt.scatter(centroids[j][2], centroids[j][1], centroids[j][0], color = 'black', marker = 'X')\n",
    "    plt.title('3D Scatter Plot')\n",
    "\n",
    "    # The second subplot to display our clustered image\n",
    "    # Displaying the image\n",
    "    new_matrix = np.full((256,256,3), 255)\n",
    "\n",
    "    # Using a for loop to fill up our clustered_img with the appropriate vales\n",
    "    for k in range(sample_idx.shape[0]):\n",
    "        colour = centroids[cluster_idx[sample_idx[k][0]][sample_idx[k][1]]]\n",
    "        x_points = sample_idx[k][0]\n",
    "        y_points = sample_idx[k][1]\n",
    "        new_matrix[x_points][y_points] = colour\n",
    "\n",
    "    # Displaying the image\n",
    "    # print(clustered_img.shape)\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(new_matrix[:,:,[2,1,0]])\n",
    "    plt.show\n",
    "    time.sleep(2)\n",
    "\n",
    "    # return the figure\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ec8ed9",
   "metadata": {
    "id": "30ec8ed9"
   },
   "source": [
    "Now modify your **mykMeans** function and create a new **myKMeans_visualize** function by adding the **visualize** function that you wrote above in order to visualize the results at every iteration. \n",
    "\n",
    "Make sure you add a time pause of 1-2 seconds after each iteration so there is enough time for you to observe the convergence of the centroids clearly! *Hint*: the '*time*' library we imported at the beginning provides a '*sleep*' function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7b328d",
   "metadata": {
    "id": "3d7b328d"
   },
   "outputs": [],
   "source": [
    "def mykMeans_visualize(data, centroids, T):\n",
    "    # Inputs  - data        : numpy array (N x d)\n",
    "    #         - centroids   : numpy array (k x d)\n",
    "    #         - T           : integer\n",
    "    # N = number of data points, d = dimension of data, k = number of centroids, T = number of iterations\n",
    "    \n",
    "    # Initialize the list to store the k-means loss at each iteration\n",
    "    kMeans_loss = np.zeros((1,T))\n",
    "    sum_dloss = np.zeros((data.shape[0],1))\n",
    "    sum_dimloss = np.zeros((1,data.shape[1]))\n",
    "\n",
    "    for t in range(T):\n",
    "\n",
    "        # Printing the total number of iterations\n",
    "        # print(\"Iterations:\", t)\n",
    "\n",
    "        # Reinitialising all of the required values\n",
    "        distances = dist2c(data, centroids)\n",
    "        cluster_idx = np.argmin(distances.T,axis = 1)\n",
    "\n",
    "        # Update centroids\n",
    "        for i in range(centroids.shape[0]):\n",
    "            indexes = np.where(cluster_idx == i)\n",
    "            centroids[i] = np.mean(data[indexes[0]], axis=0)\n",
    "\n",
    "        for n in range(data.shape[0]):\n",
    "            for k in range(centroids.shape[0]):\n",
    "                if cluster_idx[n] == k:\n",
    "                    for d in range(data.shape[1]):\n",
    "                        sum_dimloss[0][d] = (data[n][d] - centroids[0][d]) ** 2\n",
    "                    sum_dloss[n][0] = np.sum(sum_dimloss)\n",
    "\n",
    "        # Calculate the k-means loss at this iteration\n",
    "        kMeans_loss[0][t] = np.sum(sum_dloss)\n",
    "\n",
    "        # Randomization of values\n",
    "        x_idx = np.random.choice(256, size=250, replace=False)\n",
    "        y_idx = np.random.choice(256, size=250, replace=False)\n",
    "        sample_idx = np.column_stack((x_idx, y_idx))\n",
    "    \n",
    "        # Calling the function\n",
    "        f = visualize(data, cluster_idx, centroids, sample_idx)\n",
    "\n",
    "    # Return back the three values\n",
    "    return cluster_idx, centroids, kMeans_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fdb1d8",
   "metadata": {
    "id": "e0fdb1d8"
   },
   "source": [
    "Test your new **myKMeans_visualize** function on the *sharon.jpg* image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2556f19",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "executionInfo": {
     "elapsed": 124,
     "status": "error",
     "timestamp": 1649255908183,
     "user": {
      "displayName": "Markus Hiller",
      "userId": "16727612601438706693"
     },
     "user_tz": -600
    },
    "id": "e2556f19",
    "outputId": "27613132-5856-40ea-d42b-fc8bc7ed2a87"
   },
   "outputs": [],
   "source": [
    "# Specify a random seed (will determine the random initialisation)\n",
    "np.random.seed(17)\n",
    "\n",
    "# Initialising the variables\n",
    "data = img[:][1]\n",
    "\n",
    "# Looping through the image to fill in our data variable\n",
    "for i in range(img.shape[1]-1):\n",
    "    data = np.append(data,img[:][i+1], axis = 0)\n",
    "\n",
    "# Calling our random centroids function to generate 4 random centroids\n",
    "k = 4\n",
    "random_centroides = random_centroids(data,k)\n",
    "\n",
    "# Calling the mykMeans function\n",
    "T = 20\n",
    "cluster_idx, centroids, kMeans_loss = mykMeans_visualize(data, random_centroides, T)\n",
    "\n",
    "# Displaying the image\n",
    "new_matrix = np.zeros((65536,3))\n",
    "\n",
    "# Using a for loop to fill up our clustered_img with the appropriate vales\n",
    "for i in range(centroids.shape[0]):\n",
    "    indexes = np.where(cluster_idx == i)\n",
    "    new_matrix[indexes[0]] = centroids[i]\n",
    "\n",
    "# 256 equal piece removed from array\n",
    "split_arr = np.split(new_matrix, 256)\n",
    "\n",
    "# Stacking the points\n",
    "clustered_img = np.stack(split_arr, axis = 1)\n",
    "clustered_img = np.around(clustered_img).astype(int)\n",
    "\n",
    "# Displaying the image\n",
    "plt.rcParams['figure.figsize'] = [7.5, 5]\n",
    "out = plt.figure()\n",
    "plt.imshow(clustered_img[:,:,[2,1,0]])\n",
    "plt.title(\"myKMeans_Visualise function\")\n",
    "plt.show()\n",
    "\n",
    "# Printing out the values for cluster_idx , the centroids and the kMeans_loss values (Checking)\n",
    "# print(\"Printing out the values for cluster_idx, centroids and kMeans_loss\")\n",
    "# print(cluster_idx)\n",
    "# print(centroids)\n",
    "print(kMeans_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0fd62a",
   "metadata": {
    "id": "ae0fd62a"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "#### Observe how the centroids move with each iteration. Did you see any pattern in the movement of each centroid? Report and discuss your observations!\n",
    "\n",
    "\n",
    "Write your answer here\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f41938",
   "metadata": {
    "id": "c3f41938"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "## Tasks 3: k-Means++ Initialization\n",
    "    \n",
    "In this task, you will implement the k-Means++ initialization of the k-Means Clustering algorithm. First, you will write a function to generate centroids for the initialization using the k-Means++ initialization procedure (You may want to follow the steps on the lab instructions sheet, and read up on the method using the provided reference). \n",
    "\n",
    "Then you will use the generated initial centroids together with your previous k-Means Clustering implementation to cluster the pixels of the *sharon.jpg* image. Finally, you will display the results.   \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cba93e0",
   "metadata": {
    "id": "0cba93e0"
   },
   "source": [
    "Write a function **kmeanspp_centroids**, which takes a dataset and an integer value k (= number of centroids) as inputs, and generates k centroids following the k-Means++ initilization procedure. You might want to re-use your **dist2c** function here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e95e56",
   "metadata": {
    "id": "25e95e56"
   },
   "outputs": [],
   "source": [
    "def kmeanspp_centroids(data, k):\n",
    "    # Inputs - data     : numpy array (N x d)\n",
    "    #        - k        : an integer value\n",
    "    # Output - centroids: numpy array (k x d)\n",
    "    # N = number of data points, d = dimension of data, k = number of centroids\n",
    "\n",
    "   # Get the number of data points and the dimensionality of the data.\n",
    "    N = data.shape[0]\n",
    "    d = data.shape[1]\n",
    "\n",
    "    # Initialize the centroids matrix as a k-by-d matrix of zeros.\n",
    "    centroids = np.zeros((k, d))\n",
    "\n",
    "    # Choose the first centroid randomly from the data points.\n",
    "    centroids_idx = np.random.choice(data.shape[0], size=1, replace=False)\n",
    "    centroids[0] = data[centroids_idx]\n",
    "\n",
    "    # Compute the distance between each data point and the first centroid.\n",
    "    distances = dist2c(data, centroids)\n",
    "    distances = distances[0]\n",
    "\n",
    "    # Calculate the probability of choosing each data point as the next centroid.\n",
    "    probability = np.zeros(N)\n",
    "    for i in range(N):\n",
    "        probability[i] = distances[i] / np.sum(distances)\n",
    "\n",
    "    # Choose the second centroid using the calculated probabilities.\n",
    "    centroids_idx = np.random.choice(data.shape[0], size=1, replace=False, p=probability)\n",
    "    centroids[1] = data[centroids_idx]\n",
    "    \n",
    "    ndistances = np.zeros(N)\n",
    "\n",
    "    # Compute the nearest distance between each data point and the existing centroids.\n",
    "    for i in range(1, k - 1):\n",
    "        distances = dist2c(data, centroids[:i])\n",
    "        for x in range(distances.shape[1]):\n",
    "            for y in range(distances.shape[0]):\n",
    "                if y == 0:\n",
    "                    ndistances[x] = distances[y][x]\n",
    "                elif distances[y][x] < ndistances[x]:\n",
    "                    ndistances[x] = distances[y][x]\n",
    "\n",
    "        # Calculate the probability of choosing each data point as the next centroid.\n",
    "        probability = np.zeros(N)\n",
    "        for j in range(ndistances.shape[0]):\n",
    "            probability[j] = ndistances[j] / np.sum(ndistances)\n",
    "\n",
    "        # Choose the next centroid using the calculated probabilities.\n",
    "        centroids_idx = np.random.choice(data.shape[0], size=1, replace=False, p=probability)\n",
    "        centroids[i + 1] = data[centroids_idx]\n",
    "\n",
    "    # Return the generated centroids.\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4444e3",
   "metadata": {
    "id": "9d4444e3"
   },
   "source": [
    "Now, use your **kmeanspp_centroids** function to generate four random centroids. Then, use your **mykMeans** function to cluster the pixels of the *sharon.jpg* image. (You may want to reshape your image before applying the **mykMeans** function.) Display the final clustered image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1769233d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "executionInfo": {
     "elapsed": 117,
     "status": "error",
     "timestamp": 1649256170564,
     "user": {
      "displayName": "Markus Hiller",
      "userId": "16727612601438706693"
     },
     "user_tz": -600
    },
    "id": "1769233d",
    "outputId": "2d6be6ec-e414-4d9c-8cd5-c280ee6b6865"
   },
   "outputs": [],
   "source": [
    "# We first specify our random seed here\n",
    "np.random.seed(17)\n",
    "\n",
    "# Initialising the variable\n",
    "T = 20\n",
    "new_data = img[:][1]\n",
    "new_data = img.reshape(-1,3)\n",
    "\n",
    "# Run teh clustering after using the kmeans++ method to initialise the centroids\n",
    "centroids = kmeanspp_centroids(new_data,4)\n",
    "cluster_idx,centroids,kMeanspp_loss = mykMeans_visualize(new_data,centroids,T)\n",
    "\n",
    "# Display the result\n",
    "arr = np.zeros((65536,3))\n",
    "\n",
    "# Using a loop to loop through the variable\n",
    "for i in range(centroids.shape[0]):\n",
    "    indexes = np.where(cluster_idx == i)\n",
    "    arr[indexes[0]] = centroids[i]\n",
    "\n",
    "# Reconstruct the image using the clustered pixel values and display the result.\n",
    "Y = arr.reshape(img.shape)\n",
    "Y = np.around(Y).astype(int)\n",
    "plt.rcParams['figure.figsize'] = [7.5,5]\n",
    "fig = plt.figure()\n",
    "plt.imshow(Y[:,:,[2,1,0]])\n",
    "plt.title('My Kmeanspp_centroids function')\n",
    "plt.show()\n",
    "\n",
    "# Print the k-means++ loss\n",
    "print(kMeanspp_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d52ef4",
   "metadata": {
    "id": "29d52ef4"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "#### Were you able to obtain the same results (the same clustered sharon image) as you did with the random initialization, or do they differ significantly? Were you able to obtain the results faster than with the random initialization? Report your findings, and explain why you think this happens!\n",
    "\n",
    "Write your answer here\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b911aa4f",
   "metadata": {
    "id": "b911aa4f"
   },
   "source": [
    "Now, plot the loss of the two k-Means Clustering methods over the number of iterations. You may want plot the loss curves of the two methods in the same plot for convenience of comparison, and make sure to add an appropriate legend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7290df",
   "metadata": {
    "id": "5d7290df",
    "outputId": "95ffdf5f-3842-4e28-8f3d-b583bc3a7dd0"
   },
   "outputs": [],
   "source": [
    "### Insert your solution here ###\n",
    "\n",
    "# Display your losses for the random initialization vs. the kmeans++ initialization\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0,20), kMeans_loss.T, label = \"k means\", color = 'red')\n",
    "plt.plot(np.arange(0,20), kMeanspp_loss.T, label = \"k means ++\")\n",
    "plt.rcParams['figure.figsize'] = [7.5, 5]\n",
    "\n",
    "# Labelling the graph below\n",
    "plt.legend()\n",
    "plt.xlabel(\"The number of iterations\")\n",
    "plt.ylabel(\"The loss of the two k-Means Clustering methods\")\n",
    "plt.title(\"Plot of the two k-Means Clustering Methods vs the number of iterations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8475379a",
   "metadata": {
    "id": "8475379a"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "    \n",
    "    \n",
    "#### From your results, discuss the main differences between random initialization and k-Means++ initialization. What do you notice regarding the position of the initial centroids? Which one do you think is better, and why? What do you observe regarding the loss and convergence of both methods? Discuss the pros and cons of the two methods.\n",
    "\n",
    "Write your answer here\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694bdf02",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "## Tasks 4: GMM: Analysing Data Distributions.\n",
    "    \n",
    "In this task, we will learn about how data can be modelled via its distribution. We first start with a toy example, which helps us understand the concepts.\n",
    "    \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7727a5",
   "metadata": {},
   "source": [
    "### Think about your data like a pro!\n",
    "You can think of any data sample as a random vector sampled from a distribution that is usually super complicated to model analytically. Nevertheless, there are various methods to estimate the distribution of data. \n",
    "\n",
    "Let us first look at an intuitive way of thinking about this. Consider a set of images of size $100 \\times 100$ representing face images. You can think of any $100 \\times 100$ data sample as a $10,000$D random vector sampled from a distribution that we do not know. If you have an image, where you put a mouth on the forehead, this impossible image has a very low-likelihood to be generated from the distribution that YOU DO NOT KNOW. However, a regular image from say your favorite artist has a reasonable likelihood to be generated from that distribution. \n",
    "\n",
    "We can think about data as a random process that provides us with new tools and algorithms to analyze it. \n",
    "\n",
    "### The game begins\n",
    "\n",
    "OK, let us start with a toy example. We assume that our data can be modeled using a GMM with the following parameters:\n",
    "\n",
    "\\begin{align}\n",
    "    &\\mu_1 = -1, \\quad \\mu_2 = 0, \\quad \\mu_3 = 0.5, \\quad \\mu_4 = 2 \\;.\\\\\n",
    "    &\\sigma_1 = 1.0, \\quad \\sigma_2 = 2.0, \\quad \\sigma_3 = 3.0, \\quad \\sigma_4 = 1.0\\;.\\\\\n",
    "    &\\omega_1 = 0.6, \\quad \\omega_2 = 0.1, \\quad \\omega_3 = 0.1, \\quad \\omega_4 =  0.2\\;.\\\\\n",
    "\\end{align}\n",
    "\n",
    "We pretend that we do not know this is the case and will try below to estimate this distribution. But let's first take a closer look at the GMM itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2712d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Insert your solution here (use numpy arrays to define the provided parameters) ###\n",
    "# Initialising the variables\n",
    "means = np.array([-1, 0, 0.5, 2])\n",
    "stds = np.array([1.0, 2.0, 3.0, 1.0])\n",
    "weights = np.array([0.6, 0.1, 0.1, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e597f7ed",
   "metadata": {},
   "source": [
    "### Plot the pdf of a GMM\n",
    "\n",
    "Recall that a Gaussian distribution is defined as\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathcal{N}(\\mathbf{x} | \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp{\\Big(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\Big)}\\;.\n",
    "\\end{align}\n",
    "\n",
    "The pdf of a GMM with $k$ components is \n",
    "\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "    f({x}) &= \\sum_{i=1}^k \n",
    "    \\omega_i \\mathcal{N}(\\mathbf{x} | \\boldsymbol{\\mu}_i, \\boldsymbol{\\Sigma}_i) \\;.\n",
    "\\end{align}\n",
    "\n",
    "Here, ${x} \\in \\mathbb{R}$ (in our case) is the sample we evaluate the pdf at, $\\omega_i$ is the weight (i.e., mixing coefficient) of the $i$-th Gaussian component, $\\boldsymbol{\\mu}_i$ is the mean vector of the $i$-th Gaussian component, and $\\boldsymbol{\\Sigma}_i$ is the covariance matrix of the $i$-th Gaussian component. The symbol $\\mathcal{N}(\\cdot|\\boldsymbol{\\mu}_i,\\boldsymbol{\\Sigma}_i)$ denotes the probability density function of a multivariate Gaussian distribution with mean vector $\\boldsymbol{\\mu}_i$ and covariance matrix $\\boldsymbol{\\Sigma}_i$. \n",
    "\n",
    "Now, below write a code to plot the pdf of the GMM described above between [-10,10]. If your code is correct, you should see the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84902c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_pdf(x , mean , sd):\n",
    "    # The inputs and the outputs of your function should be as follows:\n",
    "    # Inputs - x : numpy array of linearly spaced points (N x 1)\n",
    "    #        - mean : an integer value\n",
    "    #        - sd: an integer value\n",
    "    # Output - prob_density : a numpy array of probability densities\n",
    "    \n",
    "    ### Insert your solution here ###\n",
    "    \n",
    "    prob_density = (1.0 / (np.sqrt(2 * np.pi) * sd)) * np.exp(-0.5 * ((x - mean) / sd)**2)\n",
    "    return prob_density\n",
    "\n",
    "# Define the GMM PDF\n",
    "def gmm_pdf(x, weights, means, sds):\n",
    "    gmm_pdf = np.zeros_like(x)\n",
    "    for weight, mean, sd in zip(weights, means, sds):\n",
    "        gmm_pdf += weight * normal_pdf(x, mean, sd)\n",
    "    return gmm_pdf\n",
    "\n",
    "# we know the GMM, so plot the PDF of it\n",
    "# Do this by creating some points in between -10 and 10 via linspace\n",
    "x = np.linspace(-10, 10, 1000)\n",
    "\n",
    "# Then calculate the likelihood (probabilities) of each point.\n",
    "# Calculate the GMM PDF\n",
    "pdf = gmm_pdf(x, weights, means, stds)\n",
    "\n",
    "# Plot the PDF\n",
    "plt.plot(x, pdf)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.title('PDF of a Gaussian Mixture Model')\n",
    "plt.show()\n",
    "# Hint: Look at the formulas above for information how to 'compose' the overall pdf using the components of the mixture.\n",
    "\n",
    "### Insert your solution here ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d4d857",
   "metadata": {},
   "source": [
    "### Next stop: Sampling from a GMM!\n",
    "\n",
    "If we have a GMM, to draw samples from it, we can do the following:\n",
    "\n",
    "1) First, sample from the components according to their weights using the random.choice function from the numpy package.\n",
    "2) Then, sample from a normal distribution based on the chosen component and according to the parameters of the normal distribution.\n",
    "\n",
    "Study the code below and convince yourself that what it does is to generate 1,000 samples from the GMM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b234db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000\n",
    "\n",
    "chosen_comp = np.random.choice(a=means.shape[0], size=num_samples, p=weights)\n",
    "\n",
    "X = np.random.normal(loc=means[chosen_comp], scale=stds[chosen_comp], size=num_samples).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3552782",
   "metadata": {},
   "source": [
    "### Hold on, what is a pdf?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414879ab-b8d4-4cda-a2f5-73bc35a9655d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "#### Note that in the majority of cases we just have raw data and we do not know the pdf, so check the following figure and explain what it shows.\n",
    "\n",
    "Write your answer here\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1cb919-05eb-461e-bab2-dde894c676a9",
   "metadata": {},
   "source": [
    "_Note_: If you cannot see the displayed image 'Task4_GMM_01_what_is_a_pdf.png', have a look into the provided 'img' folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248a4272-a6bb-4195-8535-691abf032195",
   "metadata": {},
   "source": [
    "![Task4_GMM_01_what_is_a_pdf.png](img/Task4_GMM_01_what_is_a_pdf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e66ce5",
   "metadata": {},
   "source": [
    "### Can we estimate the pdf from data? \n",
    "\n",
    "Yes, you can use a GMM to estimate the pdf or you can use a technique called the \"Kernel Density Estimation\" or KDE to estimate an arbitrary pdf. Check out the following code snippet and explain what you understand from the code below. Also, check the [KDE](https://en.wikipedia.org/wiki/Kernel_density_estimation) on wiki and learn about the algorithm (you can skip how the bandwidth can be set automatically if you want to)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab16d8ec-fe29-4597-affa-9e63cb63417d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "#### Check out the following code snippet and use your newly acquired knowledge about KDE to explain what you understand from the code below. Also take a look at the plot and explain what you see.\n",
    "\n",
    "Write your answer here\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f21bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "# There is no free-lunch. If you want to use a KDE, you need to identify appropriate hyper-parameters of \n",
    "# the algorithm, here a kernel and its parameters. We will use the Epanechnikov kernel with a bandwidth of 0.4. \n",
    "# Feel free to try other values.\n",
    "\n",
    "kde = KernelDensity(kernel='epanechnikov', bandwidth=0.4).fit(X)\n",
    "\n",
    "X0 = np.linspace(10, -10, 1000).reshape(-1,1)\n",
    "\n",
    "Z0_kde = np.exp(kde.score_samples(X0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b216cfab-10e9-48c0-9211-116973c3b74f",
   "metadata": {},
   "source": [
    "_Note_: If you cannot see the displayed image 'Task4_GMM_02_kde.png', have a look into the provided 'img' folder. The kde-results are displayed in purple colour."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb57644-1e90-4fb8-81f8-926b279abf19",
   "metadata": {},
   "source": [
    "![Task4_GMM_02_kde.png](img/Task4_GMM_02_kde.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c78779",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "## Tasks 5: Mean Shift - Data Modelling via its distribution.\n",
    "    \n",
    "In this task, you will apply the concepts learned above for using the [mean shift algorithm](https://ieeexplore.ieee.org/document/1000236) \n",
    " for image segmentation.\n",
    "    \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3d9c54",
   "metadata": {},
   "source": [
    "### Drumroll, here comes the Mean Shift 🐸\n",
    "\n",
    "The [Mean Shift (MS)](https://en.wikipedia.org/wiki/Mean_shift) algorithm is a simple algorithm to find the mode of a distribution (what is the mode of a distribution? -> Make sure to look it up if you do not already know). The algorithm is iterative and employs a kernel function to find the modes from samples of a distribution. Let $k(x,z)$ be a kernel function, measuring the similarity between $x$ and $z$ (ie., the higher $k(x,z)$, the more similar are $x$ and $z$). Then using $k(x,z)$, we iterate over\n",
    "\n",
    "\\begin{align}\n",
    "    m \\gets \\frac{ \\sum_{x_i} k(x_i,m) x_i}{\\sum_{x_i} k(x_i,m)} \\;.\n",
    "\\end{align}\n",
    " \n",
    "Then $m$ will be a mode of the distribution where $x_i$ are sampled from. Below, you will develop a simplified version of this algorithm using the so called _flat kernel_\n",
    "\n",
    "\\begin{align}\n",
    "k_{\\text{flat}}(x,z) = \n",
    "    \\begin{cases}\n",
    "    1 &\\text{if} \\;\\; \\|x-z\\|^2 \\leq h^2\\\\\n",
    "    0 &\\text{otherwise} \n",
    "    \\end{cases}\n",
    "\\;.\n",
    "\\end{align}\n",
    "\n",
    "A few comments:\n",
    "1. $h$ is a hyperparameter of this kernel (we call it the bandwidth of the kernel) and you need to set it properly for the algorithm to work.\n",
    "2. What this kernel does is simple, if the distance between $x$ and $z$ is less than $h$, then their similarity is considered to be 1, otherwise the similarity between $x$ and $z$ is zero.\n",
    "\n",
    "_Note_: Make sure to re-use your code from Task 1 to compute the squared distances you need within the kernels to save some implementation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc601630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement flat kernel\n",
    "\n",
    "def flat_kernel(centers, X, bandwidth=0.1):\n",
    "    \n",
    "    # The inputs and the outputs of your function should be as follows:\n",
    "    # Inputs - centers : the means (k x 1 numpy array), where k is the number of centres\n",
    "    #        - X : samples (i x 1 numpy array), where i is the number of samples\n",
    "    #        - bandwidth : an integer value representing \"h\" in the equation\n",
    "    # Output - K : the kernel output for each distance from the means\n",
    "    \n",
    "    ### Insert your solution here ###\n",
    "    # Explanation:\n",
    "    # The flat kernel that we want to implement is a type of window function that is equal to 1 inside the window\n",
    "    # (defined by the bandwidth 'h') and zero outside.\n",
    "\n",
    "    # We will first calculate the squared distances between the centers and the points in X\n",
    "    # 2) We will then calculate the kernel values by checking whether each squared distance is less than or equal \n",
    "    # to the square of the bandwidth. If it is, we set the corresponding kernel value to 1. Otherwise, we will\n",
    "    # set it to 0\n",
    "\n",
    "    # The kernel is used to determine the \"neighbourhood\" of each point in the space. If a point lies within a \n",
    "    # distance \"bandwidth\" of a center, it is considered to be in the \"neighbourhood\" of that center\n",
    "\n",
    "    # Note that the bandwidth h is a crucial parameter for the mean-shift algorithm. \n",
    "    # If it's set too high, the algorithm may converge to a single point that is the mean of all data points. \n",
    "    # If it's set too low, the algorithm may not converge at all or may converge to too many different points. \n",
    "    # So, choosing an appropriate value for the bandwidth is essential for the success of the algorithm.\n",
    "\n",
    "    # calculate the squared Euclidean distance between each data point and each center point\n",
    "    square_distance = dist2c(X, centers)\n",
    "\n",
    "    # create a zero-filled matrix to store the kernel density estimate\n",
    "    K = np.zeros(square_distance.shape)\n",
    "\n",
    "    # loop over each row and column of the distance matrix\n",
    "    for i in range(square_distance.shape[0]):\n",
    "        for j in range(square_distance.shape[1]):\n",
    "\n",
    "            # check if the squared distance between the data point and center point is within the kernel bandwidth\n",
    "            if square_distance[i][j] <= bandwidth ** 2:\n",
    "\n",
    "                # if it is, set the corresponding element in the kernel matrix to 1.0\n",
    "                K[i][j] = 1.0\n",
    "            else:\n",
    "\n",
    "                # if it's not, set the corresponding element in the kernel matrix to 0.0\n",
    "                K[i][j] = 0.0\n",
    "\n",
    "    # return the kernel matrix, which represents the kernel density estimate\n",
    "    return K\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc31351",
   "metadata": {},
   "source": [
    "## Playing with the Mean Shift algorithm - your turn!\n",
    "\n",
    "1. Write the code to perform the mean shift iterations on the data $X$ from the previous task. We will try 100 iterations for this example.\n",
    "2. Using samples from the GMM, and starting from $m=5$, what will be the mode of the distribution?\n",
    "3. Now start from $m=-5$. What mode do you converge to new? Do you think MS works on our toy data? \n",
    "4. Visualise the results for both cases (starting from $m=5$ and  $m=-5$) by showing the respective starting point and result after convergence plotted on top of the data $X$. </br> _Note_: Take a look at the function 'axvline' to use a vertical line for visualising the mode after convergence.\n",
    "\n",
    "_Hint_: To increase efficiency, you can perform steps 2. and 3. in one go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbff9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Insert your solution here ###\n",
    "\n",
    "# Initialising the variables\n",
    "counter = 100\n",
    "\n",
    "# Setting the bandwidth value\n",
    "bandwidth = 1.5\n",
    "\n",
    "# Initialize the mean variable 'm' as a 1-by-1 matrix filled with zeros.\n",
    "m = np.zeros((1,1))\n",
    "\n",
    "# Set the value of the first element of 'm' to 5.\n",
    "m[0] = 5\n",
    "\n",
    "# Plot the data points and an initial vertical line indicating the current value of 'm'.\n",
    "plt.figure()\n",
    "\n",
    "# Plot a vertical red line at x=m.\n",
    "plt.axvline(m[0], color=\"Red\", linestyle=\"--\")\n",
    "\n",
    "# Plot the data points as dots on the x-axis.\n",
    "plt.scatter(X, np.ones(X.shape))\n",
    "\n",
    "# Iterate over a maximum number of iterations.\n",
    "for i in range(counter):\n",
    "\n",
    "    # Compute the flat kernel values between each data point and the current value of 'm'.\n",
    "    my_kernel = flat_kernel(m, X, bandwidth).T\n",
    "\n",
    "    # Update the value of 'm' by taking a weighted average of the data points,\n",
    "    # where the weights are the kernel values.\n",
    "    for j in range(my_kernel.shape[1]):\n",
    "        m[0][j] = np.sum(np.multiply(my_kernel, X)) / np.sum(my_kernel)\n",
    "\n",
    "# Plot a vertical red line at the final value of 'm' and print its value.\n",
    "plt.axvline(m[0][j], color=\"Red\")\n",
    "print(\"m = 5 (Mode):\", m[0][0])\n",
    "\n",
    "# Repeat the above steps for a negative initial value of 'm'.\n",
    "obtained_min = np.zeros((1,1))\n",
    "obtained_min[0] = -5\n",
    "\n",
    "# Plot a vertical blue line at x=m_min.\n",
    "plt.axvline(obtained_min[0], color=\"Blue\", linestyle=\"--\")\n",
    "for i in range(counter):\n",
    "    my_kernel = flat_kernel(obtained_min, X, bandwidth).T\n",
    "    for j in range(my_kernel.shape[1]):\n",
    "        obtained_min[0][j] = np.sum(np.multiply(my_kernel, X)) / np.sum(my_kernel)\n",
    "\n",
    "# Plot a vertical blue line at the final value of 'm_min'.\n",
    "plt.axvline(obtained_min[0][j], color=\"Blue\")\n",
    "print(\"m = -5 (Mode):\", obtained_min[0][0])\n",
    "\n",
    "# Show the plot.\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c4c495",
   "metadata": {},
   "source": [
    "Plot the starting points(s), the calculated modes and the PDF. Use different colours for the two cases to better interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7375bdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Insert your solution here ###\n",
    "# Create a 1D array of x values to evaluate the PDF on.\n",
    "x = np.linspace(-10, 10, num=1000)\n",
    "\n",
    "# Create a zero-filled matrix to store the PDF evaluated at each mean.\n",
    "pdf = np.zeros((means.shape[0], x.shape[0]))\n",
    "\n",
    "# Loop over each mean and evaluate the PDF at the corresponding normal distribution.\n",
    "for i in range(means.shape[0]):\n",
    "\n",
    "    # Get the mean, standard deviation, and weight of the current normal distribution.\n",
    "    obtained_means = means[i]\n",
    "    obtained_stds = stds[i]\n",
    "    obtained_weights = weights[i]\n",
    "\n",
    "    # Compute the PDF at the current normal distribution.\n",
    "    pdf[i] = obtained_weights * normal_pdf(x, obtained_means, obtained_stds)\n",
    "\n",
    "# Sum the PDFs across all normal distributions to obtain the overall PDF.\n",
    "probability = np.sum(pdf, axis=0)\n",
    "\n",
    "# Plot the overall PDF and the modes found using the flat kernel density estimator.\n",
    "plt.figure()\n",
    "\n",
    "# Plot a vertical red line at the mode found for m=5.\n",
    "plt.axvline(m[0][j], color=\"Red\")\n",
    "\n",
    "# Plot a vertical blue line at the mode found for m=-5.\n",
    "plt.axvline(obtained_min[0][j], color=\"Blue\")\n",
    "\n",
    "# Plot the overall PDF as a curve.\n",
    "plt.plot(x, probability)\n",
    "\n",
    "# Add a title to the plot.\n",
    "plt.title(\"Probability Distribution Function using the flat kernels\")\n",
    "\n",
    "# Label the x-axis.\n",
    "plt.xlabel(\"x\")\n",
    "\n",
    "# Label the y-axis.\n",
    "plt.ylabel(\"Probability Density\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a437ab8",
   "metadata": {},
   "source": [
    "## From flat kernel to Epanechnikov\n",
    "\n",
    "The Epanechnikov kernel is a popular kernel used in the mean shift algorithm. It is defined as:\n",
    "\n",
    "\\begin{align} \n",
    "    K(x,z) = \n",
    "    \\begin{cases} \n",
    "        \\frac{3}{4} \\Big(1 - \\frac{\\|x - z\\|^2}{h^2} \\Big) &\\text{if } \\|x - z\\|^2 \\leq h^2\\;, \\\\ \n",
    "        0 &\\text{otherwise} \n",
    "    \\end{cases} \n",
    "\\end{align}\n",
    "\n",
    "\n",
    "The Epanechnikov kernel is a popular choice for the MS algorithm. It has a bounded support, with a flatter shape (compared to a Gaussian kernel), making it less sensitive to outliers.\n",
    "\n",
    "- Implement the Epanechnikov kernel and repeat the above task with this kernel instead of the flat kernel:\n",
    "\n",
    "1. Implement meanshift iterations.\n",
    "2. using samples from the GMM, and starting from $m=5$, what will be mode of the distribution?\n",
    "3. Now start from $m=-5$. Do you think MS works on our toy data? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77815a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epanechnikov_kernel(centers, X, bandwidth=0.1):\n",
    "    \n",
    "    # The inputs and the outputs of your function should be as follows:\n",
    "    # Inputs - centers : the means (m x 1 numpy array), where m is the number of centres\n",
    "    #        - X : samples (i x 1 numpy array), where i is the number of samples\n",
    "    #        - bandwidth : an integer value representing \"h\" in the equation\n",
    "    # Output - k : the kernel output for each distance from the means\n",
    "    \n",
    "    ### Insert your solution here ###\n",
    "    # Calculate the squared distances between the centers and the points\n",
    "    distances_squared = (centers - X.T)**2\n",
    "\n",
    "    # Scale the distances\n",
    "    scaled_distances = distances_squared / bandwidth**2\n",
    "\n",
    "    # Calculate the kernel values\n",
    "    K = np.where((scaled_distances <= 1), 3/4*(1 - scaled_distances), 0)\n",
    "    \n",
    "    # Return the value back\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c12bdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Insert your solution here ###\n",
    "# Initialize the variables.\n",
    "counter = 100\n",
    "bandwidth = 1.5\n",
    "m = np.zeros((1, 1))\n",
    "m[0] = 5\n",
    "\n",
    "# Create a scatter plot of the input data X.\n",
    "plt.figure()\n",
    "plt.axvline(m[0], color=\"Red\", linestyle=\"--\") # Add a vertical red dashed line at the initial mode estimate.\n",
    "plt.scatter(X, np.ones(X.shape)) # Plot the input data as points.\n",
    "\n",
    "# Compute the mode using the flat kernel density estimator with the initial mode estimate m=5.\n",
    "for i in range(counter):\n",
    "    my_kernel = flat_kernel(m, X, bandwidth).T\n",
    "    for j in range(my_kernel.shape[1]):\n",
    "        m[0][j] = np.sum(np.multiply(my_kernel, X)) / np.sum(my_kernel)\n",
    "\n",
    "# Plot the final mode estimate as a vertical red line.\n",
    "plt.axvline(m[0][j], color=\"Red\")\n",
    "print(\"Mode when m = 5:\", m[0][0])\n",
    "\n",
    "# Initialize another mode estimate.\n",
    "obtained_min = np.zeros((1, 1))\n",
    "obtained_min[0] = -5\n",
    "plt.axvline(obtained_min[0], color=\"Blue\", linestyle=\"--\") # Add a vertical blue dashed line at the initial mode estimate.\n",
    "\n",
    "# Compute the mode using the flat kernel density estimator with the new mode estimate m=-5.\n",
    "for i in range(counter):\n",
    "    my_kernel = flat_kernel(obtained_min, X, bandwidth).T\n",
    "    for j in range(my_kernel.shape[1]):\n",
    "        obtained_min[0][j] = np.sum(np.multiply(my_kernel, X)) / np.sum(my_kernel)\n",
    "\n",
    "# Plot the final mode estimate as a vertical blue line.\n",
    "plt.axvline(obtained_min[0][j], color=\"Blue\")\n",
    "print(\"Mode when m = -5:\", obtained_min[0][0])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d388252",
   "metadata": {},
   "source": [
    "Plot the starting points(s), the calculated modes and the PDF. Use different colours for the two cases to better interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedddaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Insert your solution here ###\n",
    "# Define the x range\n",
    "x = np.linspace(-10,10, num = 1000)\n",
    "pdf = np.zeros((means.shape[0],x.shape[0]))\n",
    "for i in range(means.shape[0]):\n",
    "    obtained_means = means[i]\n",
    "    obtained_stds = stds[i]\n",
    "    obtained_weights = weights[i]\n",
    "    pdf[i] = obtained_weights * normal_pdf(x, obtained_means, obtained_stds)\n",
    "probability = np.sum(pdf,axis = 0)\n",
    "\n",
    "# Plotting the figure\n",
    "plt.figure()\n",
    "plt.axvline(m[0][j], color = \"Red\")\n",
    "plt.axvline(obtained_min[0][j], color = \"Blue\")\n",
    "plt.plot(x, probability)\n",
    "plt.title(\"Probability Distribution Function using the Epanechnikov kernel\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"Probability\")\n",
    "\n",
    "# In this plot, the dashed lines represent the initial values of m and the solid lines represent the modes calculated by the mean shift algorithm. \n",
    "# The colored lines show the PDF of the GMM for the respective starting points. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d7c914",
   "metadata": {},
   "source": [
    "### Apply Mean-Shift to Segment an Image\n",
    "\n",
    "Now, we can use the MS algorithm to segment an image. To do this, we will use the implementation of the [sklearn package](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MeanShift.html) (you will soon see why). Here is what we will do:\n",
    "\n",
    "1. Given the sharon image, we first apply k-means on pixel colors in two different spaces, RGB and LAB (use opencv for the color conversion). Since we have no idea how many clusters might be in the image, pick a guess (maybe counting the number of 'dominant' colors you can see)\n",
    "\n",
    "2. Use the MS algorithm and tune its hyperparameters to achieve a reasonable result. \n",
    "\n",
    "3. How many colors does MS find in your image? If you fancy, repeat this with an image of your own choice and discuss the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a660051",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MeanShift, KMeans\n",
    "from sklearn import cluster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56c5768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image and convert it to both RGB and LAB colour space; Visualise your results!\n",
    "img = cv2.imread('sharon.jpg') # sharon\n",
    "\n",
    "## Convert to RGB\n",
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "## Convert to LAB\n",
    "img_lab = cv2.cvtColor(img, cv2.COLOR_BGR2Lab)\n",
    "\n",
    "## Visualise the results\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(121)\n",
    "plt.imshow(img_rgb)\n",
    "plt.title(\"RGB Image\")\n",
    "plt.subplot(122)\n",
    "plt.imshow(img_lab)\n",
    "plt.title(\"LAB Image\")\n",
    "plt.show()\n",
    "\n",
    "# Plotting it another way\n",
    "fig, ax = plt.subplots(1, 2, figsize = (10,5))\n",
    "ax[0].imshow(img)\n",
    "ax[0].set_title('RGB Image')\n",
    "ax[1].imshow(img_lab)\n",
    "ax[1].set_title('LAB Image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9501a100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the k-means clustering algorithm from the 'cluster' package of 'sklearn' (imported above) on both images and visualise your results\n",
    "\n",
    "### Insert your solution here ###\n",
    "# Here we guess there are 5 dominant colors\n",
    "n = 5\n",
    "kmeans_rgb = KMeans(n_clusters=n).fit(img_rgb.reshape(-1, 3))\n",
    "kmeans_lab = KMeans(n_clusters=n).fit(img_lab.reshape(-1, 3))\n",
    "\n",
    "# Display the results\n",
    "segmented_img_rgb = kmeans_rgb.cluster_centers_[kmeans_rgb.labels_].reshape(img_rgb.shape).astype(int)\n",
    "segmented_img_lab = kmeans_lab.cluster_centers_[kmeans_lab.labels_].reshape(img_lab.shape).astype(int)\n",
    "\n",
    "# Visualising the image\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(121)\n",
    "plt.imshow(segmented_img_rgb)\n",
    "plt.title(\"Segmented RGB Image\")\n",
    "plt.subplot(122)\n",
    "plt.imshow(segmented_img_lab)\n",
    "plt.title(\"Segmented LAB Image\")\n",
    "plt.show()\n",
    "\n",
    "# Updating the values for checking purposes\n",
    "kmeans_rgb = KMeans(n_clusters = 5, random_state = 17).fit(img.reshape(-1, 3))\n",
    "labels_rgb = kmeans_rgb.labels_.reshape(img.shape[:2])\n",
    "\n",
    "kmeans_lab = KMeans(n_clusters = 5, random_state = 17).fit(img_lab.reshape(-1,3))\n",
    "labels_lab = kmeans_lab.labels_.reshape(img_lab.shape[:2])\n",
    "\n",
    "# Visualise the segmented image\n",
    "fig,ax = plt.subplots(1, 2, figsize=(10,5))\n",
    "ax[0].imshow(labels_rgb)\n",
    "ax[0].set_title('Clustered RGB Image (K-Means)')\n",
    "ax[1].imshow(labels_lab)\n",
    "ax[1].set_title('Clustered LAB Image (K-Means)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bf7201",
   "metadata": {},
   "source": [
    "### Now, let's do Mean-Shift Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020273bd-1154-4568-b63c-1a0bcc83e5e8",
   "metadata": {},
   "source": [
    "Note that to speed up the process, we suggest you randomly select around 500 datapoints from the image and fit the model using this subset of datapoints. </br> You can then 'predict' the corresponding cluster centers (labels) for all other datapoints using this fitted MeanShift model. Remember you can look at the sklearn documentation for more information about the MeanShift() module </br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f6ec39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MeanShift Clustering for RGB and LAB image\n",
    "# Flatten the images\n",
    "h, w, c = img_rgb.shape\n",
    "img_rgb_flat = img_rgb.reshape(-1, c)\n",
    "img_lab_flat = img_lab.reshape(-1, c)\n",
    "\n",
    "# We are setting a fixed seed here\n",
    "np.random.seed(17)\n",
    "\n",
    "num_rand_samples = 500 # Number of random samlpes -- Feel free to tune this if needed\n",
    "\n",
    "rnd_idx = np.random.choice(h*w,num_rand_samples)  # <-- Can be used to 'select' the datapoints from the (flattened) image via indexing\n",
    "img_rgb_sample = img_rgb_flat[rnd_idx]\n",
    "img_lab_sample = img_lab_flat[rnd_idx]\n",
    "\n",
    "### Insert your solution here ###\n",
    "# Use MeanShift algorithm on the sampled data\n",
    "ms_rgb_check = MeanShift(bin_seeding=True).fit(img_rgb_sample)\n",
    "ms_lab_check = MeanShift(bin_seeding=True).fit(img_lab_sample)\n",
    "\n",
    "# Predict the labels for all data points using the fitted model\n",
    "labels_rgb_check = ms_rgb_check.predict(img_rgb_flat)\n",
    "labels_lab_check = ms_lab_check.predict(img_lab_flat)\n",
    "\n",
    "# Display the results\n",
    "segmented_img_ms_rgb = ms_rgb_check.cluster_centers_[labels_rgb_check].reshape(img_rgb.shape).astype(int)\n",
    "segmented_img_ms_lab = ms_lab_check.cluster_centers_[labels_lab_check].reshape(img_lab.shape).astype(int)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(121)\n",
    "plt.imshow(segmented_img_ms_rgb)\n",
    "plt.title(\"MeanShift Segmented RGB Image\")\n",
    "plt.subplot(122)\n",
    "plt.imshow(segmented_img_ms_lab)\n",
    "plt.title(\"MeanShift Segmented LAB Image\")\n",
    "plt.show()\n",
    "\n",
    "# Print the number of colors found\n",
    "print(\"Number of colors found in RGB image:\", len(np.unique(labels_rgb_check)))\n",
    "print(\"Number of colors found in LAB image:\", len(np.unique(labels_lab_check)))\n",
    "\n",
    "# Extra Checking\n",
    "# We are setting a fixed seed here\n",
    "np.random.seed(17)\n",
    "\n",
    "num_rand_samples = 500\n",
    "h, w = img.shape[:2]\n",
    "rnd_idx = np.random.choice(h*w,num_rand_samples)\n",
    "\n",
    "data_rgb = img.reshape(-1, 3)[rnd_idx]\n",
    "data_lab = img_lab.reshape(-1, 3)[rnd_idx]\n",
    "\n",
    "# Applying meanshift clustering to the images\n",
    "ms_rgb = MeanShift(bandwidth = 58, bin_seeding = True)\n",
    "ms_rgb.fit(data_rgb)\n",
    "labels_rgb_ms = ms_rgb.predict(img.reshape(-1, 3)).reshape(img.shape[:2])\n",
    "\n",
    "ms_lab = MeanShift(bandwidth = 58, bin_seeding = True)\n",
    "ms_lab.fit(data_lab)\n",
    "labels_lab_ms = ms_lab.predict(img_lab.reshape(-1, 3)).reshape(img_lab.shape[:2])\n",
    "\n",
    "# Visualising the images\n",
    "fig, ax = plt.subplots(1, 2, figsize = (10,5))\n",
    "ax[0].imshow(labels_rgb_ms)\n",
    "ax[0].set_title('Segmented RGB Image (Mean Shift)')\n",
    "ax[1].imshow(labels_lab_ms)\n",
    "ax[1].set_title('Segmented LAB Image (Mean Shift)')\n",
    "plt.show()\n",
    "print(\"Number of colors found in RGB image:\", len(np.unique(labels_rgb_ms)))\n",
    "print(\"Number of colors found in LAB image:\", len(np.unique(labels_lab_ms)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d720bb-52dd-4175-9687-b95df8e40bf0",
   "metadata": {},
   "source": [
    "### Visualise them all!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9552d567-69d9-4dcc-a462-01e874059d06",
   "metadata": {},
   "source": [
    "Visualise all four resulting clustered images: k-Means RGB & LAB, as well as mean shift RGB & LAB. </br> _Hint_: Make sure to display all results in the RGB space (and convert appropriately)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6058ed69-6d39-4680-bd25-84ba3933315e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising all cluster results\n",
    "\n",
    "### Insert your solution here ###\n",
    "fig,ax = plt.subplots(2,2,figsize = (10,10))\n",
    "ax[0,0].imshow(labels_rgb)\n",
    "ax[0,0].set_title('Segmented RGB Image (K-Means)')\n",
    "ax[0,1].imshow(labels_lab)\n",
    "ax[0,1].set_title('Segmented LAB Image (K-Means)')\n",
    "ax[1,0].imshow(labels_rgb_ms)\n",
    "ax[1,0].set_title('Segmented RGB Image (Mean-Shift)')\n",
    "ax[1,1].imshow(labels_lab_ms)\n",
    "ax[1,1].set_title('Segmented LAB Image (Mean-Shift)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dc60c5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "    \n",
    "    \n",
    "#### What do you notice about the mean-shift algorithm compared to the k-means for the RGB and LAB images?\n",
    "    \n",
    "When comparing the Mean Shift algorithm to K-Means for image segmentation in both RGB and LAB color spaces, I have noticed several differences:\n",
    "\n",
    "Number of clusters\n",
    "K-means requires the number of clusters to be specified in advance, whereas Mean Shift does not. Therefore, we can say that Mean Shift can potentially identify a different number of segments/dominant colors in the image than what was specified for K-means.\n",
    "\n",
    "Quality of segmentation\n",
    "Mean Shift considers the density of the feature space and finds modes of clusters, which can potentially lead to more accurate segmentation, especially if the clusters are not spherical or have different sizes.\n",
    "\n",
    "Color spaces\n",
    "RGB and LAB color spaces have different properties, which can affect the results of both K-means and Mean Shift. LAB space is designed to approximate human vision and it is more perceptually uniform than RGB. This means that a small change in a color's coordinates in LAB space corresponds to a small change in color as perceived by us. Therefore, segmentation in LAB space can potentially give more visually pleasing or intuitive results.\n",
    "\n",
    "Computational cost\n",
    "Mean Shift is typically more computationally intensive than K-means, especially for a large number of data points. This is why in many implementations, only a subset of pixels may be used for the actual Mean Shift iterations, and the rest are assigned to the nearest mode.\n",
    "\n",
    "Stability\n",
    "Mean Shift is less sensitive to initialization than K-means. K-means, being a simple iterative refinement algorithm, is highly dependent on initial cluster centroids and can fall into local minima, whereas Mean Shift, being a hill-climbing algorithm based on density estimation, is more robust in this sense.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dea836c",
   "metadata": {
    "id": "3dea836c"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "## Tasks 6: Comparison between k-Means, k-Means++ Initialization and Mean-Shift algorithm.\n",
    "    \n",
    "In this task, you will visualize the effects of random initialization, k-Means++ initialization and the Mean Shift algorithm by plotting the final clustering result of all data points together with their **final** cluster centers (using 3D scatter plots, similar to Task 2). </br> For the two k-means methods, additionally plot their **initial** centroids!\n",
    "    \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c4be3b",
   "metadata": {
    "id": "17c4be3b"
   },
   "outputs": [],
   "source": [
    "# We are setting a fixed seed here\n",
    "np.random.seed(17)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4ffd17",
   "metadata": {
    "id": "6d4ffd17"
   },
   "source": [
    "#### Use random initialization to cluster the pixels of the *sharon.jpg* image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e38301",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "executionInfo": {
     "elapsed": 694,
     "status": "error",
     "timestamp": 1649256467809,
     "user": {
      "displayName": "Markus Hiller",
      "userId": "16727612601438706693"
     },
     "user_tz": -600
    },
    "id": "a8e38301",
    "outputId": "d5a19c84-ef10-42ae-fde5-fd679f8ef782"
   },
   "outputs": [],
   "source": [
    "### Insert your solution here ###\n",
    "\n",
    "# Loading the image\n",
    "img = cv2.imread('sharon.jpg')\n",
    "\n",
    "# Randomly initialize the centroids and obtain the clustering results -- you can choose T=20 iterations to start with\n",
    "T = 20\n",
    "newdata = img.reshape(-1,3)\n",
    "centroids = random_centroids(newdata, 6)\n",
    "cluster_idx, centroids, kMeans_loss = mykMeans_visualize(newdata, centroids, T)\n",
    "\n",
    "# Initialising the new matrix\n",
    "new_matrix = np.zeros((65536, 3))\n",
    "for i in range(centroids.shape[0]):\n",
    "    indexes = np.where(cluster_idx == i)\n",
    "    new_matrix[indexes[0]] = centroids[i]\n",
    "\n",
    "clustered_img = new_matrix.reshape(img.shape)\n",
    "clustered_img = np.around(clustered_img).astype(int)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [7.5, 5]\n",
    "fig = plt.figure()\n",
    "plt.imshow(clustered_img[:,:,[2,1,0]])\n",
    "plt.title(\"Random Initialization Clustering\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ad63e6",
   "metadata": {
    "id": "80ad63e6",
    "tags": []
   },
   "source": [
    "\n",
    "#### Use k-Means++ initialization to cluster the pixels of the *sharon.jpg* image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f101cdc",
   "metadata": {
    "id": "1f101cdc",
    "outputId": "bc20ff2d-6b31-44c7-b5ac-740490f5b28e"
   },
   "outputs": [],
   "source": [
    "### Insert your solution here ###\n",
    "\n",
    "# Initialize the centroids via k-Means++ and obtain the clustering results -- you can choose T=20 iterations to start with\n",
    "# Loading the image\n",
    "img = cv2.imread('sharon.jpg')\n",
    "\n",
    "# Randomly initialize the centroids and obtain the clustering results -- you can choose T=20 iterations to start with\n",
    "T = 20\n",
    "newdata = img.reshape(-1,3)\n",
    "centroids = kmeanspp_centroids(newdata, 6)\n",
    "cluster_idx, centroids, kMeans_loss = mykMeans_visualize(newdata, centroids, T)\n",
    "\n",
    "# Initialising the new matrix\n",
    "new_matrix = np.zeros((65536, 3))\n",
    "for i in range(centroids.shape[0]):\n",
    "    indexes = np.where(cluster_idx == i)\n",
    "    new_matrix[indexes[0]] = centroids[i]\n",
    "\n",
    "clustered_img = new_matrix.reshape(img.shape)\n",
    "clustered_img = np.around(clustered_img).astype(int)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [7.5, 5]\n",
    "fig = plt.figure()\n",
    "plt.imshow(clustered_img[:,:,[2,1,0]])\n",
    "plt.title(\"k-Means++  Initialization Clustering\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f3138a",
   "metadata": {},
   "source": [
    "#### Use Mean Shift to cluster the pixels of the *sharon.jpg* image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e04f23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Insert your solution here ###\n",
    "np.random.seed(17)\n",
    "\n",
    "# Apply the mean-shift algorithm to cluster the pixels of the sharon.jpg image\n",
    "# Loading the image\n",
    "num_rand_sample = 500\n",
    "h,w = img.shape[:2]\n",
    "rnd_idx = np.random.choice(h*w, num_rand_samples)\n",
    "\n",
    "data_rgb = img.reshape(-1, 3)[rnd_idx]\n",
    "data_lab = img_lab.reshape(-1, 3)[rnd_idx]\n",
    "\n",
    "# Applying the meanshoft clustering to the RGB and LAB images\n",
    "ms_rgb = MeanShift(bandwidth=58, bin_seeding=True)\n",
    "ms_rgb.fit(data_rgb)\n",
    "labels_rgb_ms = ms_rgb.predict(img.reshape(-1, 3)).reshape(img.shape[:2])\n",
    "\n",
    "ms_lab = MeanShift(bandwidth=58, bin_seeding=True)\n",
    "ms_lab.fit(data_lab)\n",
    "labels_lab_ms = ms_lab.predict(img_lab.reshape(-1, 3)).reshape(img.shape[:2])\n",
    "\n",
    "# Visualise thhe segmented images\n",
    "fig,ax = plt.subplots(1,2, figsize = (10,5))\n",
    "ax[0].imshow(labels_rgb_ms)\n",
    "ax[0].set_title('Mean-Shift RGB Image')\n",
    "ax[1].imshow(labels_lab_ms)\n",
    "ax[1].set_title(\"Mean-Shift LAB Image\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85aeed91",
   "metadata": {
    "id": "85aeed91"
   },
   "source": [
    "Plot the generated centroids from each initialization method together with the final clustering result of the data points (by using corresponding centroid colours). You may want generate a side-by-side plot for easier comparison of the three methods (e.g. via subplots). Also make sure to clearly label your methods as well as axes in the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307fb46d",
   "metadata": {
    "id": "307fb46d",
    "outputId": "c6c6e0f0-fd07-484b-9341-5b9b5e6b639e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Insert your solution here ###\n",
    "\n",
    "# Create (sub)plot to display the final centroids and final cluster results obtained with Random Initialization, as well as the initial centroids\n",
    "\n",
    "\n",
    "# Create (sub)plot to display the final centroids and final cluster results obtained with k-Means++ Initialization, as well as the initial centroids\n",
    "\n",
    "\n",
    "# Create (sub)plot to display the final centroids and final cluster results obtained with the Mean Shift Algorithm\n",
    "\n",
    "\n",
    "# Display the results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e5146b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "    \n",
    "    \n",
    "#### What do you notice about the mean-shift clustering compared to the k-means algorithms? And how do the two k-means initializations differ?\n",
    "    \n",
    "Write your answer here\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3938c7d0-b35c-4d86-82c7-94aa70422223",
   "metadata": {},
   "source": [
    "Also observe the location of the initial centroids of random initialization and the K-means++ method!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d5af7b-9ef4-444f-82f6-e4c19cba264d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means requires a pre-specified numbers of clusters and they"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ECE_4076_5176_Lab3_kMeans_solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
